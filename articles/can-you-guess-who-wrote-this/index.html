<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="A reflection on the current state of AI text detection — why it matters, how it works, where it fails, and what questions we should really be asking." />
  <title>Can you guess who wrote this? — Alessandro Garavaglia</title>
  <link rel="stylesheet" href="../../assets/style.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css" />
</head>
<body class="article-page">

  <nav class="site-nav">
    <div class="container">
      <div class="site-nav-inner">
        <a href="../../" class="site-nav-logo">Alessandro Garavaglia<span class="site-nav-dot"> · </span>Tech Blog</a>
        <div class="site-nav-links">
          <a href="https://github.com/agaravaglia" target="_blank" rel="noopener">GitHub</a>
          <a href="https://medium.com/@ale.garavaglia" target="_blank" rel="noopener">Medium</a>
        </div>
      </div>
    </div>
  </nav>

  <header class="article-header">
    <div class="container">
      <a class="back-link" href="../../">← All articles</a>
      <div class="article-meta">
        <span class="badge badge-type">Reflection</span>
        <span class="badge badge-difficulty">Beginner</span>
        <time>August 12, 2025</time>
      </div>
    </div>
  </header>

  <main class="article-content container">
    <div class="article-title-block">
      <h1>Can you guess who wrote this?</h1>
      <p class="article-description">A reflection on the current state of AI text detection — why it matters, how it works, where it fails, and what questions we should really be asking.</p>
    </div>
    <p><strong>A brief discussion about detecting AI-generated content (and yes, AI helped me writing this).</strong></p>
<p><strong>Alessandro Garavaglia</strong> | 6 min read | Aug 12, 2025</p>
<hr />
<p>In an age where language models can produce fluent, structured, persuasive text in seconds, it's easy to lose track of authorship. Students use AI to write essays, journalists use it to outline stories, marketers automate blog posts, and even scientists experiment with machine-generated abstracts (or full articles).</p>
<p>It's no surprise that people are now asking: <strong>Can we tell whether a text was written by a human or an AI?</strong></p>
<blockquote>
<p><em>Or maybe the more honest question is: does it even matter anymore?</em></p>
</blockquote>
<p>This post isn't about offering answers or solutions. I don't have them. Instead, it's a reflection and a collection of ideas and links for those curious about the current state of AI text detection and what's at stake when we try (or fail) to draw a line between human and machine.</p>
<h3>Why it matters</h3>
<p>The ability to detect AI-generated text touches several domains, and none of them trivial:</p>
<ul>
<li>In <strong>education</strong>, it raises questions about learning, cheating, and fairness;</li>
<li>In <strong>media</strong>, it influences how we identify misinformation or synthetic content;</li>
<li>In <strong>intellectual property</strong>, it challenges traditional ideas of authorship and ownership;</li>
<li>And at a <strong>deeper level</strong>, on something human: how we assign value to communication, originality, and trust.</li>
</ul>
<h3>How detection works (when it does)</h3>
<p>The idea of detecting AI-generated text sounds simple enough, like catching plagiarism or spotting a fake photo, but in practice, it's far more complicated. Language is nuanced, and AI tools are increasingly good at imitating the ambiguity, rhythm, and imperfections of human writing.</p>
<h4>Language model-based detection</h4>
<p>This is one of the more intuitive approaches. The idea is to ask: how likely is it that this text was generated by an AI like GPT-4?</p>
<p>Tools like <a href="https://detectgpt.com">DetectGPT</a> use a model similar to the one suspected of generating the text. They analyze the <strong>perplexity</strong>, that a statistical measure of how predictable the text is. AI tends to generate more "typical" language, so a low-perplexity score may hint that the model wrote it.</p>
<p>However, this method is fragile:</p>
<ul>
<li>It requires access to the original model or one very similar;</li>
<li>Paraphrasing or editing even slightly can significantly reduce the model's confidence;</li>
<li>It doesn't generalize well across domains or styles.</li>
</ul>
<p>Still, it's a key technique that underpins early efforts to identify AI authorship.</p>
<h4>Stylometric and statistical analysis</h4>
<p>This method has roots in authorship attribution research that predates modern AI. It involves <strong>analyzing writing style</strong>: syntax patterns, vocabulary richness, sentence length, and punctuation habits.</p>
<p>These models are trained on known samples of human and AI writing and then use statistical classifiers (like logistic regression or neural networks) to make predictions about new text.</p>
<p>Some AI detectors like <a href="https://originality.ai">Originality.AI</a> use this approach, combined with fingerprinting techniques. But stylometry isn't always reliable:</p>
<ul>
<li>AI models can intentionally mimic human quirks;</li>
<li>Non-native speakers or people with certain writing styles may be unfairly flagged;</li>
<li>Short text or informal language often lacks enough stylistic data to make a clear prediction.</li>
</ul>
<h4>Machine Learning Classifiers</h4>
<p>Some tools, like <a href="https://gptzero.me">GPTZero</a>, take a hybrid approach: they combine <strong>linguistic features</strong> (like sentence complexity or burstiness) with <strong>AI-specific patterns</strong> and train a classifier.</p>
<p>These tools are easy to deploy and can process large volumes of content quickly, but they often suffer from the classic machine learning problem: they perform well on the training data and less so in the wild.</p>
<p>Their performance drops when:</p>
<ul>
<li>Text is edited by a human after generation.</li>
<li>The model they were trained to detect differs from the one that actually generated the content.</li>
<li>Writing style varies widely from their training dataset.</li>
</ul>
<h4>Watermarking</h4>
<p>This method is the most elegant in theory and perhaps the most ambitious. Watermarking involves embedding hidden patterns into the text at the time of generation, for example, by subtly favoring certain word choices over others in a statistically detectable way.</p>
<p>These patterns are invisible to readers but detectable by specialized algorithms. Think of it as invisible ink for digital language.</p>
<p>However, watermarking has major limitations:</p>
<ul>
<li>It only works if the model used supports watermarking.</li>
<li>Paraphrasing or translating the text can easily erase the watermark.</li>
<li>Open-source or rogue models won't implement it.</li>
</ul>
<p>Watermarking remains largely experimental at this stage, though it's being seriously considered by organizations like OpenAI and Anthropic.</p>
<h4>Zero-Shot and Contrastive Approaches</h4>
<p>Recent research has explored zero-shot detection models (like <a href="https://arxiv.org/abs/2305.15047">Ghostbuster</a>) and contrastive training (like <a href="https://arxiv.org/abs/2310.08903">DeTeCtive</a>), which aim to identify AI-generated content without access to the specific model used or without needing labeled training data.</p>
<p>These approaches are promising because they try to generalize better across styles, domains, and models. They rely on fine-tuned LLMs or contrastive learning to distinguish "machine-ness" as a latent feature of text.</p>
<h3>The gaps (and why they matter)</h3>
<p>Here's where things get complicated. In real-world contexts, detection is often unreliable:</p>
<ul>
<li><strong>False negatives:</strong> Paraphrased or lightly edited AI content frequently escapes detection. A study from the University of Maryland explores this challenge in depth: <a href="https://arxiv.org/abs/2304.04736">Is AI-generated content actually detectable?</a>.</li>
<li><strong>False positives:</strong> Human-written text, especially from non-native speakers or atypical writing styles, is sometimes flagged as AI. This has real consequences in education and hiring. The Guardian documented how this misidentification has affected students.</li>
</ul>
<p><a href="https://www.turnitin.com">Turnitin</a>, one of the most widely used academic tools, reports a 15% false negative rate in its own AI detection feature (higher education).</p>
<h3>Detection-as-policy</h3>
<p>Despite limitations, detection systems are being adopted. Universities, publishers, and companies are integrating them into workflows, sometimes transparently, often not.</p>
<p>That's led to a patchwork of practices: in some cases, tools help flag suspicious patterns; in others, they act as black boxes with real-world consequences.</p>
<ul>
<li><strong>Universities</strong> may use AI detectors to flag suspicious essays, sometimes requiring students to submit drafts or source materials. In some cases, detection results can lead to academic investigations or penalties, even when the evidence is inconclusive.</li>
<li><strong>Publishers</strong> may screen submitted articles or manuscripts for AI-generated content to maintain editorial integrity, especially in journalism or scientific publishing. Some editors use these tools as a first pass to decide if a deeper human review is needed.</li>
<li><strong>Employers</strong> might scan job application materials or writing samples for AI usage, aiming to ensure authenticity in candidates' work. However, this raises questions about fairness and potential bias against non-native speakers or those with unconventional writing styles.</li>
<li><strong>Social media platforms</strong> and <strong>content moderation teams</strong> explore detection as part of combating misinformation or synthetic content, though the scale and variety of content make reliable detection a challenge.</li>
</ul>
<p>These examples show how AI detection is evolving from a purely technical challenge to a complex social and ethical issue. The debate around their ethical use (how transparent organizations should be, how to balance trust and verification, and how to avoid unfair consequences) is ongoing.</p>
<h3>So what can we take away?</h3>
<p>The line between AI and human writing is becoming increasingly blurry, and perhaps that's the point:</p>
<blockquote>
<p><em>Detection might not be about catching someone, but about asking better questions.</em></p>
</blockquote>
<ul>
<li><strong>Was this written with the help of AI?</strong></li>
<li><strong>Should that be disclosed?</strong></li>
<li><strong>What context matters most: intention, domain, or audience?</strong></li>
</ul>
<p>AI is not inherently good or bad. It's a tool, and like any tool, the ethics depend on how we use it, and whether we're honest about that use.</p>
<p><em>If nothing else, it's worth paying attention.</em></p>
<h3>Useful resources</h3>
<p>Here's a short list of resources and references that helped inform this reflection:</p>
<ul>
<li><a href="https://arxiv.org/abs/2303.07205">Survey on Detection Techniques</a></li>
<li><a href="https://arxiv.org/abs/2301.11305">Probabilistic Detection Method</a></li>
<li><a href="https://arxiv.org/abs/2305.15047">Zero-shot Detection</a></li>
<li><a href="https://arxiv.org/abs/2310.08903">Contrastive Detection Approach</a></li>
<li><a href="https://arxiv.org/abs/2306.15666">Testing of detection tools for AI-generated text</a></li>
<li><a href="https://arxiv.org/abs/2304.04736">Is AI-generated content actually detectable?</a></li>
<li><a href="https://www.theguardian.com/technology/2023/jan/13/chatgpt-ai-tools-teachers-detect-cheating">Professors cautious of tools to detect AI-generated writing</a></li>
<li><a href="https://www.theguardian.com/technology/2023/may/27/university-ai-cheating-crisis">'I received a first but it felt tainted and undeserved': inside the university AI cheating crisis</a></li>
<li><a href="https://www.theguardian.com/commentisfree/2023/apr/09/theres-no-simple-solution-to-universities-ai-worries">There's no simple solution to universities' AI worries</a></li>
</ul>
<hr />
<p><em>This post is not meant to provide definitive answers or prescriptive guidance. Its goal is to spark reflections and discussions on the topic.</em></p>
<p><em>The views expressed are my own.</em></p>
<hr />
<p><strong>Tags:</strong> AI, Responsible AI, AI Ethics, Generative AI Tools</p>
  </main>

  <footer class="article-footer">
    <div class="container">
      <div class="article-footer-inner">
        <div class="article-footer-tags">
          <span class="label">Tags</span>
          <span class="tag-static">AI</span> <span class="tag-static">Responsible AI</span> <span class="tag-static">AI Ethics</span>
        </div>
        <a class="back-link" href="../../">← All articles</a>
      </div>
    </div>
  </footer>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>

</body>
</html>
